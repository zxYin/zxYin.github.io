<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="author" content="Zixin Yin">
  <meta name="description" content="Zixin Yin's Homepage">
  <meta name="keywords" content="Zixin Yin,殷子欣,homepage,主页,computer vision,xiaobing.ai,XJTU,Xi'an Jiaotong University,HKUST,Hong Kong University of Science and Technology,talking head synthesis,representation learning,facial landmark detection,3D face reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="ZGzK6r6fLSJ5KADM0BOqlDLqo2ij9W8IaM7XO_kHHlU">
  <title>Zixin Yin (殷子欣)'s Homepage</title>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zixin Yin (殷子欣)</name>
              </p>
              <p style="text-align:center">
                Email: zixin.yin[at]connect.ust.hk &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=bF7vffsAAAAJ&hl=en">Google Scholar </a>&nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/zxYin">Github</a>
              </p>
              <p>
                I am currently a PhD student at
                <a href="https://cse.hkust.edu.hk">The Hong Kong University of Science and Technology</a>,
                under the supervision of
                <a href="https://seng.hkust.edu.hk/about/people/faculty/lionel-ming-shuan-ni">Prof. Lionel Ni</a> (president of HKUST-gz)
                and
                <a href="https://www.microsoft.com/en-us/research/people/hshum/">Prof. Harry Shum</a> (former executive vice president of Microsoft),
                since 2021.
              </p>
              <p>
                From April 2023 to April 2025, I served as the co-founder of <a href="">Morph Studio</a>.
                Prior to that, I worked closely with
                <a href="https://sites.google.com/site/zjuwby/?pli=1">Baoyuan Wang</a>
                and
                <a href="https://dorniwang.github.io">Duomin Wang</a>
                as a research intern at
                <a href="https://www.linkedin.com/company/xiaobing-ai">Xiaobing.ai</a> starting in 2022.
                My research interests include image and video generation, visual editing, and talking head synthesis.
              </p>
              <p>
                From August 2019 to May 2021, I worked with
                <a href="https://people.eecs.berkeley.edu/~sequin/">Prof. Carlo H. Séquin</a>
                at UC Berkeley on the graphics and CAD project
                <a href="https://jipcad.github.io/">JIPCAD</a>.
              </p>
              <p>
                I received my B.S. from the Department of Computer Science and Technology
                (Honors Science Program) and completed the Honors Youth Program (少年班) at
                <a href="https://bjb.xjtu.edu.cn/">Xi'an Jiaotong University</a> in 2021.
              </p>
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/zixin.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zixin.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images/consistedit.jpg' style="width:100%;max-width:100%; position: absolute;">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</papertitle>
              <br>
              <strong>Zixin Yin</strong>, Ling-Hao Chen, Lionel M. Ni, Xili Dai
              <br>
                <em>SIGGRAPH Asia</em>, 2025,
              <br>
              <a>[PDF]</a>
              <a>[Project]</a>
              <a>[Code(coming soon)]</a>
              <a>[BibTeX]</a>
              <br>
              <p>ConsistEdit is a training-free attention control method for MM-DiT that enables precise, structure-aware image and video editing. It supports multi-round edits with strong consistency and achieves state-of-the-art performance without manual design or test-time tuning.</p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images/colorctrl.png' style="width:100%;max-width:100%; position: absolute;">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</papertitle>
              <br>
              <strong>Zixin Yin</strong>, Xili Dai, Ling-Hao Chen, Deyu Zhou, Jianan Wang, Duomin Wang, Gang Yu, Lionel M. Ni, Lei Zhang, Heung-Yeung Shum
              <br>
                <em>arXiv</em>, 2508.09131,
              <br>
              <a href="https://arxiv.org/abs/2508.09131">[PDF]</a>
              <a href="https://zxyin.github.io/ColorCtrl">[Project]</a>
              <a>[Code(coming soon)]</a>
              <a>[BibTeX]</a>
              <br>
              <p>We introduce ColorCtrl, a training-free method for text-guided color editing in images and videos. It enables precise, word-level control of color attributes while preserving geometry and material consistency. Experiments on SD3, FLUX.1-dev, and CogVideoX show that ColorCtrl outperforms existing training-free and commercial models, including GPT-4o and FLUX.1 Kontext Max, and generalizes well to instruction-based editing frameworks.</p>
          </td>
        </tr>
        <tr><td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video playsinline="" autoplay="" loop="" preload="" muted="" style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src="images/speakervid.mp4">
                </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>SpeakerVid-5M: A Large-Scale High-Quality Dataset for audio-visual Dyadic Interactive Human Generation</papertitle>
            <br>
            Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, <strong>Zixin Yin</strong>, Xili Dai, Gang Yu, Xiu Li
            <br>
            <em>arXiv</em>, 2507.09862,
            <br>
            <a href="https://arxiv.org/abs/2507.09862">[PDF]</a>
            <a href="https://dorniwang.github.io/SpeakerVid-5M/">[Project]</a>
            <a href="https://github.com/Dorniwang/SpeakerVid-5M-Code">[Code]</a>
            <a href="https://huggingface.co/datasets/dorni/SpeakerVid-5M-Dataset">[Dataset]</a>
            <br>
            <p>We introduce SpeakerVid-5M, the first large-scale dataset designed specifically for the audio-visual dyadic interactive virtual human task.</p>
          </td>
        </tr>
          <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/thpad.mp4'>
                    </video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</papertitle>
                <br>
                Zhentao Yu*, <strong>Zixin Yin*</strong>, Deyu Zhou*, Duomin Wang, Finn Wong, Baoyuan Wang
                <br>
                <em>2023 IEEE International Conference on Computer Vision</em>, ICCV 2023,
                <br>
                <a href="https://arxiv.org/abs/2212.04248">[PDF]</a>
                <a href="https://zxyin.github.io/TH-PAD">[Project]</a>
                <a href="">[Code(coming soon)]</a>
                <a href="images/thpad.txt">[BibTeX]</a>
                <br>
                <p>We introduce a simple and novel framework for one-shot audio-driven talking head generation. Unlike prior works that require additional driving sources for controlled synthesis in a deterministic manner, we instead probabilistically sample all the holistic lip-irrelevant facial motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the input audio while still maintaining both the photo-realism of audio-lip synchronization and the overall naturalness.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/pdfgc.mp4'>
                    </video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</papertitle>
                <br>
                Duomin Wang, Yu Deng, <strong>Zixin Yin</strong>, Heung-Yeung Shum, Baoyuan Wang
                <br>
                <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023,
                <br>
                <a href="https://arxiv.org/abs/2211.14506">[PDF]</a>
                <a href="https://dorniwang.github.io/PD-FGC/">[Project]</a>
                <a href="https://github.com/Dorniwang/PD-FGC-inference">[Code]</a>
                <a href="images/pdfgc.txt">[BibTeX]</a>
                <br>
                <p>We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression.
                     We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them.</p>
            </td>
        </tr>

        <tr></tr>
        <td>
          <em>(* means equal contribution)</em>
        </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://dorniwang.github.io">Duomin Wang</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
